---
title: "Annex for Term Paper (BAN440)"
output:
  html_document:
    df_print: paged
date: "2023-04-25"
pandoc_args:
- "-V"
- geometry:margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Annex 1
```{r, message = FALSE}
# Load libraries
options(scipen = 999)
library(tidyverse)
library(rnaturalearth) # provides a map of countries of the entire world
library(rworldmap) # for map generation
library(sf)  # for map generation
library(giscoR)
library(readxl) # loading excel files
library(readr)
library(MASS) # for modelling
library(knitr)
library(dplyr)
library(fastDummies)
library(patchwork)
source("functions.R") # own functions used to fasten coding (see Annex 2)
```

## Data loading and preparation
```{r loading relevant data, message=FALSE, warning=FALSE}
# restaurants <- read_csv("data/european_restaurants.csv") # original TripAdvisor data 
# gerset <- restaurants %>% filter(country == "Germany") # Subset of OG data for Germany
# write.csv(gerset, "data/gerset.csv") # save data for Germany to reduce computing time
pop <- read_excel("population.xlsx") # population for the country that is assessed (Source: Eurostats)
touristic_df <- read_csv("tourist_nut2.csv") # data on touristic movemet in Europe (Source: Eurostats)
nuts3 <- st_read("NUTS_RG_20M_2021_3035.shp/NUTS_RG_20M_2021_3035.shp", quiet = TRUE) # NUT3 information (Source: Eurostats)
nuts2 <- nuts3 # for NUT2 information
nut3area <- read_csv("reg_area3.csv") # for area information on nut3 (Source: Eurostats)
nuts3 <- nut3area %>% dplyr::select(geo, OBS_VALUE) %>%
  rename(area = OBS_VALUE, NUTS_ID = geo) %>% 
  merge(., nuts3, by = "NUTS_ID")
geom2 <- nuts3 %>% subset(LEVL_CODE == 3 & CNTR_CODE == "DE") %>% dplyr::select(NUTS_ID, geometry) %>% unique()
gerset <- read_csv("gerset.csv") # Data set for Germany as a subset of the TripAdvisor data
```

Based on the data set for Germany that was created as a simple subset of the TripAdvisor set, we create a new data set, including the NUT3 regions of Germany based on the latitude and longitude of each restaurant.
```{r, Nut2 df}
ger_allinfo = fn_create_nut(gerset, "DE")
info_set <- ger_allinfo
```

## Descriptive Statistics I
First, we needed to view the data we are using for the following research:

```{r, Plot1, message = FALSE}

plot1_set <-info_set %>% group_by(NUTS_ID) %>% dplyr::select(restaurants_per_inhab) %>% 
        unique() %>% merge(., geom2, all = TRUE)
Plot1 <- fn_plot_rest(plot1_set, "1: All NUT3 regions") # Restaurants in Germany per 100,000 inhabitants
ggsave("Plot1.png", plot = Plot1, width = 6, height = 4, dpi = 300)
```

```{r}
Plot1
```

## Filtering based on tourism data
After several tries on how to aggregate the data and cutting it into suitable markets, we used Eurostats data on the number of tourists in Germany to identify less touristic regions to ensure to have markets that are less influenced by people that are not included in our population data. 

```{r, tourism data}
toursitic_21 <- touristic_df %>% subset(TIME_PERIOD == 2021) %>% # concentrating on year TripAdvvisor data was sourced
        subset(., grepl("^DE", geo)) %>% 
        dplyr::select(geo, OBS_VALUE)
tnew <- toursitic_21[nchar(toursitic_21$geo) == 4, ] # filtering for appropriate level of aggregation
min <- tnew[order(tnew$OBS_VALUE), ][1:20, ] # 20 least touristic regions in Germany
smallest_values <- subset(tnew, OBS_VALUE %in% min$OBS_VALUE)
tourismfilter <- as.list(unique(smallest_values$geo))
working_set <- ger_allinfo[grepl(paste(tourismfilter, collapse = "|"), ger_allinfo$NUTS_ID),] # filter data for regions
```

## Data filtering for descriptive stats II and modelling
Based on the now filtered data we started to create our first regression set. The grouping and filtering of the data follows in parts the Lab3 from the lecture (BAN-440 summer term 2023, NHH Bergen Norway)
```{r, first regression set}
working_set$n_rest <- working_set$n_rest %>% as.numeric()
working_set$avg_rating <- working_set$avg_rating %>% as.numeric()
regression_set <- working_set %>%
                group_by(NUTS_ID) %>%
                summarise(
                        Population = (mean(Population)) / 1000,
                        MOUNT_TYPE = mean(MOUNT_TYPE),
                        URBN_TYPE = mean(URBN_TYPE),
                        COAST_TYPE = mean(COAST_TYPE),
                        n_rest = mean(n_rest),
                        area = mean(area)) %>%
                subset(area < 700 & Population < 500) # important second filter for smaller regions see map 3 in paper
```


## Descriptive Statistics II
For the paper we decided to create a short but informative overview on the German restaurant industry, based on the TripAdvisor data. Therefore and adding to the Plot1, two more plots were created showing the two steps for filtering the data down to what is used in the models.

```{r Plot2, message = FALSE}
plot2_set <- working_set %>% 
        group_by(NUTS_ID) %>% 
        dplyr::select(restaurants_per_inhab) %>% 
        unique() %>% merge(., geom2, all = TRUE)
Plot2 <- fn_plot_rest(plot2_set, "2: Less tourisitc regions") # second descriptive plot

plot3_set <- working_set %>% 
        subset(area < 700 & Population < 500000) %>% # third descriptive plot
        group_by(NUTS_ID) %>% 
        dplyr::select(restaurants_per_inhab) %>% 
        unique() %>% merge(., geom2, all = TRUE)

Plot3 <- fn_plot_rest(plot3_set, "3: Smaller and less dense markets") # filtered to smaller and less populated markets

all_plots <- Plot1 + Plot2 + Plot3 + plot_layout(ncol = 3) + 
        plot_annotation(title = "Restaurants in Germany per 100,000 residents, cut down for modeling:")
# ggsave("all_plots.png", plot = all_plots, width = 15, height = 6, dpi = 300)

```

```{r}
all_plots
```

## Final Touches on Data
In this step, we have now extracted more information from the TripAdvisor data set. First, we turned to the "cuisine" column. For the analysis of the markets and restaurants contained in them, the information about which cuisine is probably very helpful. Second, we were interested in the "info_tag" column. This column contains various information that briefly describes the restaurant in question. In particular, we were interested in the tag "cheap", as this information could be informative for the analysis of the market. The filtered data was then transferred to the working_set, to finally end up in the regression_set in compressed form.

```{r}
# get cuisines as Boolean variable 
ger_info_cuisine <- ger_allinfo %>% 
        dplyr::select(restaurant_link, NUTS_ID, cuisines) %>% 
        separate_rows(cuisines, sep = ", ") %>%
        mutate(new_col = 1) %>%
        pivot_wider(names_from = cuisines, values_from = new_col, values_fill = 0)

ger_info_cuisine_nut3 <- ger_info_cuisine %>% 
        dplyr::select(! restaurant_link) %>% 
        group_by(NUTS_ID) %>% 
        summarise_all(sum)

# get info_tag as Boolean variable
ger_info_tag <- ger_allinfo %>% dplyr::select(restaurant_link, NUTS_ID, top_tags) %>% 
        separate_rows(top_tags, sep = ", ") %>%
        mutate(new_col = 1) %>%
        pivot_wider(names_from = top_tags, values_from = new_col, values_fill = 0) %>%
        mutate_at(c("Cheap Eats"), as.numeric) %>%
        rename(cheap_eats = "Cheap Eats") %>% 
        dplyr::select(NUTS_ID, cheap_eats)

# extract the info about cheap food
cheap_eats <- ger_info_tag %>% 
        group_by(NUTS_ID) %>% 
        summarise(cheap = sum(cheap_eats))

regression_set <- regression_set %>% 
        merge(., ger_info_cuisine_nut3, by = "NUTS_ID") %>%
        merge(., cheap_eats, by = "NUTS_ID")

# create now a data set that is used for modeling
# only including data that is needed in the later models
regression_set_short <- regression_set %>% 
        dplyr::select(NUTS_ID, n_rest, Population, area, cheap, 
                      URBN_TYPE, MOUNT_TYPE, COAST_TYPE, Italian, Cafe, German)
```

## Descriptive Statistics III (looking at the cut data)
```{r}
regression_set_short2 <- regression_set %>% 
        dplyr::select(NUTS_ID, n_rest, Population, area, cheap, 
                      URBN_TYPE, MOUNT_TYPE, COAST_TYPE, Italian, German, European, Pizza)

regression_set_short2$n_rest <- as.numeric(regression_set_short$n_rest)
cols <- c("Population", "n_rest", "area")
summary_modeldata <- summary(dplyr::select(regression_set_short2, cols))
summary_set <- info_set %>% group_by(NUTS_ID) %>% summarise(Population = mean(Population)/1000,
                                                            n_rest = mean(n_rest),
                                                            area = mean(area))
summary_wholegermany <- summary(dplyr::select(summary_set, cols))
print(summary_wholegermany)
print(summary_modeldata)
# Print table using knitr
kable(summary_modeldata, format = "latex")

regression_set$Italian <- as.numeric(regression_set$Italian)
colSums(Filter(is.numeric, regression_set))
vect <- head(sort(colSums(Filter(is.numeric, regression_set[,9:120])), decreasing = TRUE), 15)
vect

#sum(regression_set$`NA`) / sum(vect) 

# Seems there is a lot of variance in the number of banks and population sizes, but there are a lot of markets small enough to apply the BR model.

#Look at raw correlations (not the perfect measure given that number of banks is not continuous but will be a decent approximation):
regression_set$n_rest <- as.numeric(regression_set$n_rest)
regression_set %>% filter(!is.na(Population) & area<Inf & Population>0) %>% summarize(rho=cor(Population,n_rest))
```

## BR Model Run
```{r}
new_set <- regression_set
new_set$n_rest <- as.numeric(new_set$n_rest)
table(new_set$n_rest)
upperb = 200

# creating the regression data set by adding dummy variables
brdata <- new_set %>%
        mutate(nrest = as.factor(ifelse(n_rest <= upperb, n_rest,upperb)))

brdata <- dummy_cols(brdata, select_columns="nrest")

brdata2 <- brdata
varn <- names(brdata %>% dplyr::select(starts_with("nrest_")))

# filling the rows with 1 until the true value
for (i in 1:nrow(brdata2)) {
        row_i <- as.matrix(brdata2[i, varn])
        first_one_col <- match(1, row_i)  # index of the first 1 in the row
        if (!is.na(first_one_col)) {
                brdata2[i, (varn[1:(first_one_col - 1)])] <- 1
        }
}

```

```{r}
upperb2 = 42

brformula1<-as.formula(paste0("nrest ~ Population + Population:(",paste(varn[1:upperb2+1], collapse = " + "),") + area"))
# brformula1 
model_BR1<-polr(brformula1, data=brdata2,  method="probit")
# summary(model_BR1)

brformula2<-as.formula(paste0("nrest ~ Population + Population:(",paste(varn[1:upperb2+1], collapse = " + "),") + area + URBN_TYPE + Healthy + Italian + cheap + Cafe + German"))
# brformula2 
model_BR2<-polr(brformula2, data=brdata2,  method="probit")
# summary(model_BR2)
```
Unfortunately, both models returned the same warning message. It indicates that in both models the variables we use to predict the number of restaurants in the markets are to correlated, and therefore in some instance can explain "too much". Furthermore, this lead to errors in the summaries of both models (for the sake of readability these were not included, but the warning message returned by the summary function was "Warning: NaNs produced")

## GV Model Run
Now, we turned to the second option from the lecture and recreated and fine tuned the following models:


### Model 1
```{r}
# create a vector of breaks for grouping
breaks <- c(0, 50, 70, 90, 110, 130, 140, 165, 190, 220, 250, 300, 450, Inf)

# create a vector of labels for the groups
labels <- c("0-50", "50-70", "70-90", "90-110", "110-130", "130-140", "140-165", "165-190", "190-220", "220-250", "250-300", "300-450", "+450")

regression_set$n_rest <- as.integer(regression_set$n_rest)
# use cut function to group the n_rest column
regression_set$group <- cut(regression_set$n_rest, breaks = breaks, labels = labels)

# calculate the average for each group
aggregate(area ~ group, data = regression_set, FUN = mean)
```

```{r}
# Regression ####
regression_set$n_rest <- regression_set$n_rest %>%as.factor()
model1 <- polr(group ~ log(Population) + area, data = regression_set, method = "probit")
summary(model1)

upperb <- 12 ## Number of sections, can be modified

lambda<-model1$coefficients # Estimates
theta<-model1$zeta # Cutoffs

S_N <- exp(theta - mean(regression_set$area)*lambda[2])

slab<-NULL
for (i in 1:(upperb)) {slab[i]<-paste0("$S_",i,"$")}
names(S_N)<-slab

ETR_N <- exp(theta[2:upperb] - theta[2:upperb-1]) * (1:(upperb-1))/(2:upperb)

elab<-NULL
for (i in 2:upperb) {elab[i-1]<-paste0("$s_",i,"/s_",i-1,"$")}
names(ETR_N)<-elab


knitr::kable(S_N, col.names = c("'000s"), digits = 4,
             caption = 'Entry thresholds',
             booktabs = TRUE)

knitr::kable(ETR_N, col.names = c("ETR"), digits = 4,
             caption = 'Entry threshold ratios',
             booktabs = TRUE)
```

### Model 2
```{r}
model2 <- polr(group ~ log(Population) + URBN_TYPE + MOUNT_TYPE + COAST_TYPE, 
               data = regression_set, method = "probit")
summary(model2) # This model returned slightly better thresholds

upperb <- 12 ## Number of sections, can be modified
lambda<-model2$coefficients # Estimates
theta<-model2$zeta # Cutoffs

S_N <- exp(theta - mean(regression_set$URBN_TYPE)*lambda[2] - mean(regression_set$MOUNT_TYPE)*lambda[3] - mean(regression_set$COAST_TYPE)*lambda[4])

slab<-NULL
for (i in 1:(upperb)) {slab[i]<-paste0("$S_",i,"$")}
names(S_N)<-slab

ETR_N <- exp(theta[2:upperb] - theta[2:upperb-1]) * (1:(upperb-1))/(2:upperb)

elab<-NULL
for (i in 2:upperb) {elab[i-1]<-paste0("$s_",i,"/s_",i-1,"$")}
names(ETR_N)<-elab

knitr::kable(S_N, col.names = c("'000s"), digits = 4,
             caption = 'Entry thresholds',
             booktabs = TRUE)

knitr::kable(ETR_N, col.names = c("ETR"), digits = 4,
             caption = 'Entry threshold ratios',
             booktabs = TRUE)
```


### Model 3
```{r}
table(regression_set_short$Italian)
# create a vector of breaks for grouping
breaks <- c(0, 5, 10, 15, 20, 30, 40, Inf)

# create a vector of labels for the groups
labels <- c("0-5", "6-10", "11-15" , "16-20", "21-30", "31-40", "40+")

regression_set_short$Italian <- as.numeric(regression_set_short$Italian)
regression_set_short$groupI <- cut(regression_set_short$Italian, breaks = breaks, labels = labels)
```

```{r}
regression_set_short$Italian <- as.factor(regression_set_short$Italian)
model3 <- polr(groupI ~ log(Population) + cheap + URBN_TYPE + MOUNT_TYPE + COAST_TYPE + German, data = regression_set_short, method = "probit")
summary(model3)
upperb <- 6 ## Number of sections, can be modified

lambda<-model3$coefficients # Estimates
theta<-model3$zeta # Cutoffs

S_N <- exp(theta - mean(regression_set_short$cheap)*lambda[2] - mean(regression_set_short$URBN_TYPE)*lambda[3] - mean(regression_set_short$MOUNT_TYPE)*lambda[4] - mean(regression_set_short$COAST_TYPE)*lambda[5]  - mean(regression_set_short$German)*lambda[6])

slab<-NULL
for (i in 1:(upperb)) {slab[i]<-paste0("$S_",i,"$")}
names(S_N)<-slab

ETR_N <- exp(theta[2:upperb] - theta[2:upperb-1]) * (1:(upperb-1))/(2:upperb)

elab<-NULL
for (i in 2:upperb) {elab[i-1]<-paste0("$s_",i,"/s_",i-1,"$")}
names(ETR_N)<-elab


knitr::kable(S_N, col.names = c("'000s"), digits = 4,
             caption = 'Entry thresholds',
             booktabs = TRUE)

knitr::kable(ETR_N, col.names = c("ETR"), digits = 4,
             caption = 'Entry threshold ratios',
             booktabs = TRUE)
```

### Model 4
```{r}
table(regression_set_short$German)
# create a vector of breaks for grouping
breaks <- c(0, 10, 20, 30, 40, 50, 60, 70, 90, Inf)

# create a vector of labels for the groups
labels <- c("0-10", "11-20", "21-30", "31-40", "41-50", "51-60", "61-70", "71-90", "90+")

regression_set_short$German <- as.numeric(regression_set_short$German)
regression_set_short$groupG <- cut(regression_set_short$German, breaks = breaks, labels = labels)
```

```{r}
regression_set_short$groupG <- as.factor(regression_set_short$groupG)
regression_set_short$Italian <- as.numeric(regression_set_short$Italian)
model4 <- polr(groupG ~ log(Population) +  cheap + URBN_TYPE + MOUNT_TYPE + COAST_TYPE + Italian, data = regression_set_short, method = "probit")
summary(model4)

upperb <- 8 ## Number of sections, can be modified

lambda<-model4$coefficients # Estimates
theta<-model4$zeta # Cutoffs

S_N <- exp(theta - mean(regression_set_short$cheap)*lambda[2] - mean(regression_set_short$URBN_TYPE)*lambda[3] - mean(regression_set_short$MOUNT_TYPE)*lambda[4] - mean(regression_set_short$COAST_TYPE)*lambda[5]  - mean(regression_set_short$Italian)*lambda[6])

slab<-NULL
for (i in 1:(upperb)) {slab[i]<-paste0("$S_",i,"$")}
names(S_N)<-slab

ETR_N <- exp(theta[2:upperb] - theta[2:upperb-1]) * (1:(upperb-1))/(2:upperb)

elab<-NULL
for (i in 2:upperb) {elab[i-1]<-paste0("$s_",i,"/s_",i-1,"$")}
names(ETR_N)<-elab


knitr::kable(S_N, col.names = c("'000s"), digits = 4,
             caption = 'Entry thresholds',
             booktabs = TRUE)

knitr::kable(ETR_N, col.names = c("ETR"), digits = 4,
             caption = 'Entry threshold ratios',
             booktabs = TRUE)
```


# Annex 2
Here you find the functions that were used to create the data set for the analysis, and the plots. The first funtion was very helpful when researching the TripAdvisor data set on different country levels, espacially for France, Italy, and Spain. 
```{r}
# creating a function to assign the NUT3 level information to the data based on longitude and latitude
fn_create_nut = function(Data_Set, Country) {
        nut3 <- Data_Set %>% dplyr::select(restaurant_link, latitude, longitude) %>% na.omit()
        nut3 <- nut3  %>% 
                sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326)  %>% 
                sf::st_join(giscoR::gisco_nuts) %>% 
                subset(LEVL_CODE == 3)
        
        n_rest <- nut3 %>% 
                subset(., CNTR_CODE == Country & LEVL_CODE == 3) %>% # Country specific settings here
                group_by(NUTS_ID) %>% 
                summarise(n_rest = n()) %>%
                st_drop_geometry(.)
        
        nut_clean <- subset(nut3, select = c(restaurant_link, NUTS_ID)) %>% 
                st_drop_geometry(.)
        
        nuts_merged <- nut_clean %>%
                merge(., nuts3, by = "NUTS_ID", all = TRUE) %>%
                merge(., n_rest, by = "NUTS_ID", y.all = TRUE) %>%
                subset(., CNTR_CODE == Country & LEVL_CODE == 3) # Country specific settings here
        
        country_allinfo <- merge(nuts_merged, Data_Set , by = "restaurant_link", y.all = TRUE) %>% 
                dplyr::select(., !province) %>%
                rename(province = NAME_LATN)
        
        country_allinfo <- country_allinfo %>%
                merge(.,pop, by = c("province"))
        
        country_allinfo$Population <- as.integer(country_allinfo$Population)
        country_allinfo <- country_allinfo %>%
                mutate(restaurants_per_inhab = (n_rest/Population)*100000)
        
        return(country_allinfo)
}
# creating a function to plot the data for a certain threshold (number of restaurants per 100,000 inhabitants)
fn_plot_rest = function(Data_Set, Title) {
        rest_plot <- ggplot(data = Data_Set, aes(geometry = geometry)) +
                geom_sf() +
                geom_sf(data = Data_Set, aes(fill = restaurants_per_inhab)) +
                scale_colour_gradient(low = "#132B43", high = "#56B1F7",
                                      space = "Lab", na.value = "grey50", 
                                      breaks = c(0, 100, 200, 500, 1000, 2000),
                                      aesthetics = "colour",
                                      name = "") +
                guides(fill = guide_colorbar(title = NULL)) +
                labs(title = Title, color = NULL) +
                theme(axis.title.x=element_blank(),
                      axis.text.x=element_blank(),
                      axis.ticks.x=element_blank(),
                      axis.title.y=element_blank(),
                      axis.text.y=element_blank(),
                      axis.ticks.y=element_blank())
        return(rest_plot)
}
```

